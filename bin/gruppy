#!/usr/bin/env python

"""
 This is Gruppy.

 Purpose

  Organization is using ELK stack in a very AWS-intensive world.
  Organization wants to use S3 as much as humanly possible, rather
  than beats/filebeat, whatever. They have their reasons.

  Challenge: S3 doesn't behave quite like most *NIX filesystems.
  Apparently it doesn't play well with the default "file" 
  input mechanism that comes with logstash out of the box,
  especially with respect to keeping track of which data
  has already been read and which has yet to be read.

  An S3 plugin for logstash exists. Articles warn us, though,
  that the algorithm(s) it employs for the sake of keeping
  track of which log files have already been read and which
  haven't, and where we are/were in the file, may involve 
  brutal, inefficient, ugly force. 

  If you're using the logstash S3 plugin and it's working
  well for you, please disregard gruppy and move along,
  nothing to see here.

  Gruppy avoids logstash native file codec wonkiness with
  respect to S3 by using cat and the logstash stdin codec.
  Gruppy tracks which logs have already been processed 
  via a sqlite3 database.

  WARNING: this is offered as is, with no assurance of
  suitability or workability. You should be aware that
  no special precautions have been taken to secure, for 
  instance, the database that gruppy uses.
"""
import sqlite3
import sys
import shlex
import subprocess
from subprocess import Popen, PIPE
import os
import fnmatch
from string import Template
from datetime import datetime
from shutil import copyfile
import argparse
import logging
import time
import atexit
import yaml
import warnings
import tempfile
#import pprint

__author__ = "Wutufubapoobah"
__copyright__ = "Copyright 2017, Wutufubapoobah"
__credits__ = ["Wutufubapoobah"]
__license__ = "GPL"
__version__ = "0.1.2"
__maintainer__ = "utufubapoobah"
__email__ = "wutufubapoobah@gmail.com"
__status__ = "Development"

# some useful constants
APPNAME = 'gruppy'
PROGNAME = '%s' % (APPNAME) # synonym
VERSION = '0.1.2'
PIDFILE = '/var/run/%s.pid' % (APPNAME)
CFGFILE = '/etc/%s/%s.yml' % (APPNAME,APPNAME)
AWSBIN = '/root/.local/bin/aws' # should test existence in precondition check
CTGZJSON2KVRECORDSBIN = '/bin/cvt-cloudtrail-gz-json-to-kv.py'
# sleeptime in seconds
SLEEPTIME = 2

# feed type constants
FT_UNKNOWN = 0
FT_FILESYSTEM = 1
FT_S3_VIA_AWSCLI = 2

# a few globals: cfg, logger, args
cfg = {}
logger = None
args = None

#pp = pprint.PrettyPrinter(indent=4)

class FSPath(object):
	"""
	A FSPath object

	Attributes:
	 path 
	 feed_name
	 pattern
	"""
	def __init__(self,path,feed_name,pattern):

		self.path_template = path
		self.feed_name = feed_name
		if pattern == None:
			self.pattern = '*'
		else:
			self.pattern = pattern

	def __repr__(self):
		return """
<%s:
 %-20s: %s
 %-20s: %s
 %-20s: %r
>""" % ("FSPath","path_template",self.path_template,"feed_name",self.feed_name,"pattern",self.pattern)

	def find_files(self):
		"""
		Return a list of files found under path that match self.pattern,
		where path is obtained through parameter substitution from self.path_template
		"""

		path = ''
		if '$' in self.path_template:

			# perform template parameter substitution on path
			t = Template(self.path_template)
			path = t.substitute(THIS_YEAR = datetime.now().year, 
				THIS_MONTH = '%02d' % (datetime.now().month), 
				THIS_DATE = '%02d' % (datetime.now().day))

		else:
			path = self.path_template

		msg = "seeking files in path %s" % (path)
		logger.info(msg)
		#print "seeking files in path %s" % (path)
		# first, verify that path exists
		matches = []
		for root, dirnames, filenames in os.walk(path):
			for filename in fnmatch.filter(filenames, self.pattern):
				logger.debug("joining root (%s) to fname (%s)" % (root,filename))
				matches.append(os.path.join(root, filename))

		files_sorted_by_ctime = sorted(matches, key=os.path.getctime)
		return files_sorted_by_ctime

	def find_files_on_s3_via_awscli(self):
		"""
		Find files on s3 in target directory by calling 'aws s3 ls <path>'
		where path is obtained through parameter substitution from self.path_template
		e.b. 
		aws s3 ls s3://hap-log-qa/hap-qa-cloudtrail/AWSLogs/011448383031/CloudTrail/us-east-1/2017/06/12/
		"""
		path = ''
		if '$' in self.path_template:

			# perform template parameter substitution on path
			t = Template(self.path_template)
			path = t.substitute(THIS_YEAR = datetime.now().year, 
				THIS_MONTH = '%02d' % (datetime.now().month), 
				THIS_DATE = '%02d' % (datetime.now().day))

		else:
			path = self.path_template

		msg = "seeking files in path %s" % (path)
		logger.info(msg)
		# assert that aws is found at /root/.local/bin/aws
		cmd = "%s s3 ls s3:/%s/" % (AWSBIN, path)
		logger.info("cmd used to retrieve s3 filelist via awscli: %s" % (cmd))
		try:
			process = Popen(shlex.split(cmd), stdout=PIPE)
			(output, err) = process.communicate()
			exit_code = process.wait()
		except Exception, e:
			msg = "failed to retrieve s3 filelist via awscli: %s" % (str(e))
			logger.error(msg)
			return None

		# otherwise output contains a string, split into list of lines
		lines = output.splitlines()
		# but we only care about filenames, which are final element of multicolumn line
		filepaths = []
		# columns are date, time, size, filename
		for line in lines:
			filepaths.append("%s/%s" % (path,line.split()[3]))

		return filepaths

class FeedFilter(object):
	"""
	A FeedFilter object

	Attributes:
	 name - string
	 negate - boolean
	 grep_regex_pattern - string
	 feed_name - string
	 enabled - boolean
	"""
	def __init__(self,name,negate,grep_regex_pattern,feed_name,enabled):

		self.name = name
		self.negate = negate
		self.grep_regex_pattern = grep_regex_pattern
		self.feed_name = feed_name
		self.enabled = enabled

	def __repr__(self):

		if self.negate == False:
			negate_me = "False"
		else:
			negate_me = "True"
		if self.enabled == False:
			enable_me = "False"
		else:
			enable_me = "True"
		return """
<%s:
 %-20s: %s
 %-20s: %s
 %-20s: %s
 %-20s: %s
 %-20s: %s
>""" % (
	"FeedFilter",
	"name",self.name,
	"regex pattern",self.grep_regex_pattern,
	"feed name",self.feed_name,
	"negate?",negate_me,
	"enabled?",enable_me)		

class Feed(object):
	"""
	A Logstash input feed

	Attributes:
	 id - integer
	 name - text
	 type - integer (see the FT_<type> feed type constants!)
	 config_filespec
	 enabled - 0 (false) or 1 (true)
	 transform - 0 (false) or 1 (true)
	 transformation

	 Note: transformation is defined as an external command, script or program that 
	 would be run with the data file as sole argument. 

	 How transformations work is that ordinarily, input feed processes input file in place.
	 When tranform == True, though, we copy input file to temp directory,
	 apply the transformation, then input using the tranformed copy.
	"""
	def __init__(self,name,feedtype,cfgfile,enabled,transform,transformation):
		"""initialize the object"""

		self.name = name
		self.type = int(feedtype)
		self.cfgfile = cfgfile
		self.enabled = False # disabled by default
		if enabled == 0:
			self.enabled = False
		elif enabled == 1:
			self.enabled = True
		else:
			raise Exception('Feed init received invalid value for "enabled" attribute')
		self.transform = False
		if transform == 1:
			self.transform = True
		self.transformation = transformation
		self.filters = []

	def __repr__(self):
		ftstr = ''
		if self.type == FT_UNKNOWN:
			ftstr = "unknown"
		elif self.type == FT_FILESYSTEM:
			ftstr = "filesystem"
		elif self.type == FT_S3_VIA_AWSCLI:
			ftstr = "s3-via-awscli"
		else:
			raise Exception("invalid feed type contant value: %s" % (self.type))

		return """
<%s:
 %-20s: %s
 %-20s: %s
 %-20s: %s
 %-20s: %r
 %-20s: %r
 %-20s: %s
>""" % ("Feed","name",self.name,"type", ftstr,"cfgfile",self.cfgfile,"enabled?",
	self.enabled,"transform?",self.transform,"transformation",self.transformation)

	def is_valid_config_file(self):
		""" verify that self.cfgfile is a valid logstash config file """

		msg = "checking to see if %s has a valid config file, please wait ... " % (self.name)
		logger.info(msg)
		#sys.stdout.write("checking to see if %s has a valid config file, please wait ... " % (self.name))
		#sys.stdout.flush()
		#/usr/share/logstash/bin/logstash --path.settings /etc/logstash -f /usr/share/logstash/conf/elb-consolidated-logstash.conf -t
		
		logger.debug('%s --path.settings /etc/logstash -f %s -t' % (cfg['logstashbin'],self.cfgfile))
		cmd = '%s --path.settings /etc/logstash --path.data %s -f %s -t' % (cfg['logstashbin'],cfg['tempdir'],self.cfgfile)
		process = Popen(shlex.split(cmd), stdout=PIPE)
		process.communicate()
		exit_code = process.wait()
		# send newline
		#print
		if exit_code != 0:
			msg = "invalid config"
			logger.error(msg)
			#print "invalid config"
			return False

		msg = "config ok"
		logger.info(msg)
		#print "config ok"
		return True

	def fetch_interesting_paths(self):
		""" fetch interesting paths from the database """

		logger.info("fetching interesting paths for feed %s" % (self.name))

		try:
			conn = sqlite3.connect(cfg['dbfilespec'])
		except Exception, e:
			msg = "Connect fail for db %s: %s" % (cfg['dbfilespec'],str(e))
			logger.error(msg)
			return None

		c = conn.cursor()

		# get list of feeds and their properties
		query_get_interesting_paths = "SELECT * FROM interesting_paths WHERE feed_name = '%s'" % (self.name)
		try:
			c.execute(query_get_interesting_paths)
		except Exception, e:
			msg = "Execute fail on db %s for query %s: %s" \
			     % (cfg['dbfilespec'],query_get_interesting_paths,str(e))
			logger.error(msg)
			conn.close()
			return None

		# fetchall returns a list of tuples
		rows = []
		try:
			rows = c.fetchall()
		except Exception, e:
			msg = "Fetchall fail on db %s for query %s: %s" \
			    % (cfg['dbfilespec'],query_get_interesting_paths,str(e))
			logger.error(msg)
			conn.close()
			return None

		interesting_paths = []
		for r in rows:
			arglist = list(r)
			interesting_paths.append(FSPath(*arglist))

		conn.close()

		return interesting_paths

	def fetch_filters(self):
		""" return a dictionary of filters in which key is the filter name """
		try:
			conn = sqlite3.connect(cfg['dbfilespec'])
		except Exception, e:
			print "Connect fail for db %s: %s" % (cfg['dbfilespec'],str(e))
			return None
		c = conn.cursor()
		sql = "SELECT f.name,f.negate,f.grep_regex_pattern,x.feed_name,x.enabled FROM feed_x_filter AS x,filters AS f WHERE x.feed_name = '%s' and x.filter_name = f.name" % (self.name)" % (self.name)
		try:
			c.execute(sql)
		except Exception, e:
			print "Execute fail on db %s for query %s: %s" % (cfg['dbfilespec'],sql,str(e))
			conn.close()
			return None
		# fetchall returns a list of tuples
		rows = []
		try:
			rows = c.fetchall()
		except Exception, e:
			print "Fetchall fail on db %s for query %s: %s" % (cfg['dbfilespec'],query_get_feeds,str(e))
			conn.close()
			return None

		filters = {}
		for r in rows:
			filter_name = r[0]
			arglist = list(r)
			filters[filter_name] = FeedFilter(*arglist)
		conn.close()
		logger.debug(filters)
		self.filters = filters
		return filters

def exit_handler():
	""" things to do before we go """

	try:
		os.removedirs(cfg['tempdir'])
	except Exception, e:
		pass

	try:
		os.remove(PIDFILE)
	except Exception, e:
		pass

def convert_loglevstr_to_loglevint(loglevstr):
	""" returns logging.NOTSET if we fail to match string """

	if loglevstr.lower() == "critical":
		return logging.CRITICAL
	if loglevstr.lower() == "error":
		return logging.ERROR
	if loglevstr.lower() == "warning":
		return logging.WARNING
	if loglevstr.lower() == "info":
		return logging.INFO
	if loglevstr.lower() == "debug":
		return logging.DEBUG
	return logging.NOTSET

def convert_loglevint_to_loglevstr(loglevint):
	""" if no match, returns None """

	if loglevint == logging.CRITICAL:
		return "critical"
	if loglevint == logging.ERROR:
		return "error"
	if loglevint == logging.WARNING:
		return "warning"
	if loglevint == logging.INFO:
		return "info"
	if loglevint == logging.DEBUG:
		return "debug"
	if loglevint == logging.NOTSET:
		return "notset"
	return None

def init_configuration():

	global cfg
	cfg['dbfilespec'] = '/var/lib/%s/%s.db' % (PROGNAME,PROGNAME)
	cfg['logdir'] = '/var/log/%s' % (PROGNAME)
	cfg['logfile'] = '%s/%s.log' % (cfg['logdir'],PROGNAME)
	cfg['loglevel'] = logging.INFO
	cfg['logstashbin'] = '/usr/share/logstash/bin/logstash'

	config_read_succeeded = False
	try:
		with open(CFGFILE,'r') as f:
			cfg_file_data = yaml.safe_load(f)
			config_read_succeeded = True
	except Exception, e:
		msg = "unable to read configfile {%s}, proceeding with defaults: %s\n" \
			 % (CFGFILE,str(e))
		warnings.warn(msg)
		pass

	if config_read_succeeded == True and not cfg_file_data == None:
		if 'database.filespec' in cfg_file_data:
			cfg['dbfilespec'] = cfg_file_data['database.filespec']
		if 'logging.directory' in cfg_file_data:
			cfg['logdir'] = cfg_file_data['logging.directory']

		""" revoking ability to configure logfile """
		#if 'logging.filespec' in cfg_file_data:
		#	cfg['logfile'] = cfg_file_data['logging.filespec']
		if not args.use == None:
			cfg['logfile'] = "%s/%s_%s.log" % (cfg['logdir'],APPNAME,args.use)
		else:
			cfg['logfile'] = "%s/%s.log" % (cfg['logdir'],APPNAME)

		if 'logging.level' in cfg_file_data:
			""" set cfg['loglevel'] to constant int value """
			retval = convert_loglevstr_to_loglevint(cfg_file_data['logging.level'])
			if not retval == logging.NOTSET:
				cfg['loglevel'] = retval
		if 'logstash.program' in cfg_file_data:
			cfg['logstashbin'] = cfg_file_data['logstash.program']

	return

def init_arguments():

	global args
	argparser = argparse.ArgumentParser(
		description="Realtime logstash input processing that supports S3")
	argparser.add_argument("-i","--interval", type=int, 
		help="set polling interval (in seconds)")
	argparser.add_argument("-u","--use", 
		help="specify input feed to use by name")
	argparser.add_argument("-v","--version", 
		action="store_true", default=False, help="show version")
	argparser.add_argument("-m", "--more", 
		action="store_true", default=False, help="more information")
	argparser.add_argument("-s", "--silent", 
		action="store_true", default=False, help="disable logging and messages")
	args = argparser.parse_args()

	return

def init_logging():

	global logger

	# if main logfile doesn't exist, create it
	logdir = os.path.dirname(cfg['logfile'])
	if not os.path.isdir(logdir):
		try:
			os.mkdir(logdir)
		except Exception, e:
			msg = "unable to create logdir: %s" % (str(e))
			print "%s" % (msg)

	if not os.path.isfile(cfg['logfile']):
		try:
			with open(cfg['logfile'], 'a'):
				os.utime(cfg['logfile'], None)
		except Exception, e:
			msg = "unable to create logfile {%s}: %s" % (cfg['logfile'],str(e))
			# can't log the error! should print this to stderr
			print "%s" % (msg)

	# logfile = '/var/log/fetch_%s_data.log' % (feed.name)
	logger = logging.getLogger(__name__)
	logger.setLevel(cfg['loglevel'])
	# create file handler for logging
	handler = logging.FileHandler(cfg['logfile'])
	handler.setLevel(cfg['loglevel'])
	# create a logging format
	formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
	handler.setFormatter(formatter)
	# add the handler to the logger
	logger.addHandler(handler)
	print "Logging to %s" % (cfg['logfile'])
	logger.info("starting")
	retval = convert_loglevint_to_loglevstr(logging.getLogger().getEffectiveLevel())
	if not retval == None:
		logger.info("log level: %s" % (retval))
	else:
		logger.info("log level: unknown")

	return

def init_pidfile():

	# write pid to pidfile
	pid = str(os.getpid())
	try:
		pidfile = open(PIDFILE, 'w')
		pidfile.write(pid)
		pidfile.close()
	except Exception, e:
		msg = "unable to write process id {%s} to pidfile {%s}: %s" \
		  % (pid,PIDFILE, str(e))
		logger.warning(msg)
	return

def init():
	""" everything we want to do to prepare for success """

	atexit.register(exit_handler)

	# argparser
	init_arguments()

	# set default configurable values
	init_configuration()

	# set up logging
	init_logging()

	init_pidfile()

	# create temporary directory used by this process
	if os.path.isdir('/dev/shm'):
		cfg['tempdir'] = tempfile.mkdtemp(dir='/dev/shm')
	else:
		cfg['tempdir'] = tempfile.mkdtemp()

	return

def fetch_feeds():
	""" return a dictionary of feeds in which key is feed name """

	try:
		conn = sqlite3.connect(cfg['dbfilespec'])
	except Exception, e:
		print "Connect fail for db %s: %s" % (cfg['dbfilespec'],str(e))
		return None

	c = conn.cursor()

	# get list of feeds and their properties
	query_get_feeds = 'SELECT * FROM feeds'
	try:
		c.execute(query_get_feeds)
	except Exception, e:
		print "Execute fail on db %s for query %s: %s" \
		 % (cfg['dbfilespec'],query_get_feeds,str(e))
		conn.close()
		return None

	# fetchall returns a list of tuples
	rows = []
	try:
		rows = c.fetchall()
	except Exception, e:
		print "Fetchall fail on db %s for query %s: %s" \
		% (cfg['dbfilespec'],query_get_feeds,str(e))
		conn.close()
		return None

	feeds = {}
	for r in rows:
		# name is the first element in tuple
		feed_name = r[0]
		arglist = list(r)
		feeds[feed_name] = Feed(*arglist)
		feeds[feed_name].fetch_filters()

	conn.close()
	logger.debug(feeds)
	return feeds

def fetch_a_feed(feed_name):
	""" fetch an input feed """

	logger.info("fetching input feed %s" % (feed_name))
	try:
		conn = sqlite3.connect(cfg['dbfilespec'])
	except Exception, e:
		msg = "Connect fail for db %s: %s" % (cfg['dbfilespec'],str(e))
		logger.error(msg)
		return None

	c = conn.cursor()

	# get list of feeds and their properties
	query_get_feed = "SELECT * FROM feeds WHERE name = '%s'" % (feed_name)
	try:
		c.execute(query_get_feed)
	except Exception, e:
		msg = "Execute fail on db %s for query %s: %s" \
		     % (cfg['dbfilespec'],query_get_feed,str(e))
		logger.error(msg)
		conn.close()
		return None

	# fetchone returns a tuple
	row = None
	try:
		row = c.fetchone()
	except Exception, e:
		msg = "Fetchone fail on db %s for query %s: %s" \
		    % (cfg['dbfilespec'],query_get_feed,str(e))
		logger.error(msg)
		conn.close()
		return None

	if row == None:
		# no feed found with that name
		conn.close()
		return None
	# should assert that first element in tuple matches feed_name
	column_list = list(row)
	obj = Feed(*column_list)

	conn.close()
	logger.debug(obj)

	# fetch feed's filter(s)
	obj.fetch_filters()
	return obj


def file_already_processed(filespec):
	""" determine whether file was already processed, return T/F (on error, return None) """

	try:
		conn = sqlite3.connect(cfg['dbfilespec'])
	except Exception, e:
		msg = "Connect fail for db %s: %s" % (cfg['dbfilespec'],str(e))
		logger.error(msg)
		return None

	c = conn.cursor()

	# get list of feeds and their properties
	query_get_already_processed = "SELECT * FROM files_successfully_processed WHERE filespec = '%s'" \
	  % (filespec)

	try:
		c.execute(query_get_already_processed)
	except Exception, e:
		msg = "Execute fail on db %s for query %s: %s" \
		     % (cfg['dbfilespec'],query_get_already_processed,str(e))
		logger.error(msg)
		conn.close()
		return None

	# fetchone returns a list of tuples
	rows = []
	try:
		rows = c.fetchall()
	except Exception, e:
		msg = "Fetchall fail on db %s for query %s: %s" % (cfg['dbfilespec'],query_get_already_processed,str(e))
		logger.error(msg)
		conn.close()
		return None

	if len(rows) == 0:
		conn.close()
		return False

	if len(rows) > 0:
		conn.close()
		return True

	# wtf? len(rows) returned negative?
	# should this be an exception? don't know what's going on beyond this point
	msg = "len(rows) returned a negative value"
	logger.warning(msg)
	#warnings.warn(msg)
	conn.close()
	return None

# Callers should always check the return value!
# args: filespec, transformation (a string)
# algorithm:
#  copies filespec to tempdir,
#  transforms the copy by applying the transformation to the copy in tempdir
#  returns filespec of copy in tempdir
# return values:
#  on success, returns tempfile
#  on failure, returns None
# 
def transform_input_data(filespec,transformation):
	""" transform file before using it as input """

	# verify that transformation is a file that exists
	if not os.path.exists(transformation):
		logger.error("transformation external program not found: %s" % (transformation))
		return None

	# and is executable

	# ideally, we'd safely and securely create tempfile using python tempfile module
	# for the sake of velocity and to simplify diagnostics and troubleshooting,
	# we're going to use original filename for now.
	tempfile = '/tmp/%s' % (os.path.basename(filespec))
	copyfile(filespec, tempfile)

	# now apply transformation
	#msg = "applying transformation, please wait ... "
	#sys.stdout.write("applying transformation, please wait ... ")
	#sys.stdout.flush()
	cmd = '%s %s' % (transformation,tempfile)
	logger.info("transformation: %s" % (cmd))
	exit_code = -1
	try:
		process = Popen(shlex.split(cmd), stdout=PIPE)
		process.communicate()
		exit_code = process.wait()
	except Exception, e:
		msg = "failed to transform feed file: %s" % (str(e))
		logger.error(msg)

	# send newline
	#print
	if exit_code != 0:
		msg = "transformation returned a non-zero value"
		logger.warning(msg)
		#print "transformation returned a non-zero value"
		return None

	msg = "transformation complete"
	logger.info(msg)
	#print "tranformation complete"
	return tempfile

def copy_s3_file_to_tempdir_using_awscli(filespec):
	""" returns tempfilespec on success, None on fail """
	msg = "copying s3 file to tempdir using awscli"
	logger.debug(msg)

	tempfile = '/tmp/%s' % (os.path.basename(filespec))
	cmd = '%s s3 cp s3:/%s %s' % (AWSBIN,filespec,tempfile)
	logger.debug("cmd: %s" % (cmd))
	exit_code = 0
	try:
		process = Popen(shlex.split(cmd), stdout=PIPE)
		process.communicate()
		exit_code = process.wait()
	except Exception,e:
		msg = "failed to copy s3 file via aws cli %s: %s" % (filespec,str(e))
		logger.error(msg)
	# send newline
	#print
	if exit_code != 0:
		# if tempfile exists, should verify integrity via checksum
		# if exists and checksum matches, okay, ignore exit code
		# if tempfile exists and checksums don't match, raise warning
		msg = "copy s3 file via aws cli returned a non-zero value"
		logger.warning(msg)
		#print "transformation returned a non-zero value"
		return None

	return tempfile

def process_s3_file_using_awscli(feed,filespec):
	""" 
	Not supporting transformations currently on s3 via awscli
	1. aws s3 cp s3://path $tempfile
	2. run this command => CTGZJSON2KVRECORDSBIN $tempfile | logstash -f feed.cfgfile 
	"""
	msg = "using the %s feed to process s3 file %s via awscli, please wait ... " % (feed.name,filespec)
	logger.info(msg)

	tempfile = copy_s3_file_to_tempdir_using_awscli(filespec)
	if tempfile == None:
		msg = "copy_s3_file_to_tempdir_using_awscli returned None, cannot continue"
		logger.error(msg)
		return None

	cmdlog = '%s/fetch_%s_data.log' % (cfg['logdir'],feed.name)
	logger.info('command output is being written to %s' % (cmdlog))
	cmd = '%s %s | %s --path.settings %s --path.data %s -f %s >> %s 2>&1' \
	   % (CTGZJSON2KVRECORDSBIN, tempfile, cfg['logstashbin'], '/etc/logstash', cfg['tempdir'], feed.cfgfile, cmdlog)

	logger.debug('cmd: %s' % (cmd))
	retval = 0
	try:
		retval = subprocess.call(cmd, shell=True)

	except Exception,e:
		msg = "failed to logstash %s using %s: %s" % (tempfile, CTGZJSON2KVRECORDSBIN, str(e))
		logger.error(msg)
		return False
	# send newline
	#print
	if retval != 0:
		msg = "external command to process file returned a non-zero value: %s" % (cmd)
		logger.warning(msg)
		#print "transformation returned a non-zero value"
		#return False

	# otherwise success, delete tempfile and update the database
	os.remove(tempfile)
	return True

def preprocess_feed_file_as_required(feed,filespec):
	""" preprocess the feed file as required 
	returns filespec of file to be processed
	"""
	use_this_datafile = filespec 
	if feed.transform == True:
		retval = transform_input_data(filespec,feed.transformation)
		if retval == None:
			msg = "transformation failed, skipping data file %s" % (filespec)
			logger.error(msg)
			#warnings.warn("\n%s" % (msg))
			return False
		# if retval not None, it should be tempfile
		use_this_datafile = retval

	# verify that data file exists before processing it?
	if not os.path.isfile(use_this_datafile):
		msg = "data file missing, unable to process it: %s" % (use_this_datafile)
		logger.warning(msg)
		#warnings.warn("\n%s" % (msg))
		return None

	return use_this_datafile

def postprocess_feed_file_as_required(feed,filespec):
	""" postprocess the feed file as required """

	if feed.transform == True:
		# before removing, make sure it's tempfile, NOT original!
		dirname = os.path.dirname(use_this_datafile)
		if dirname == '/tmp':
			try:
				os.remove(use_this_datafile)
			except Exception, e:
				msg = "failed to remove transformed datafile %s: %s" \
				  % (use_this_datafile, str(e))
				logger.warning(msg)
		else:
			msg = "expecting to remove transformed datafile in tempdir but datafile not tempfile: %s" % (use_this_datafile)
			logger.warning(msg)
			#warnings.warn("\n%s" % (msg))

	return		

def build_command_for_processing_feed_file(feed,use_this_datafile):
	""" build command string for processing a feed file """

	cmd = None
	cmdlog = '%s/fetch_%s_data.log' % (cfg['logdir'],feed.name)
	logger.info('command output is being written to %s' % (cmdlog))

	if args.silent == False and args.more == True:
		cmd = 'cat -v %s | %s --path.settings /etc/logstash --path.data %s -f %s >> %s' \
		% (use_this_datafile, cfg['logstashbin'], cfg['tempdir'], feed.cfgfile, cmdlog)

	elif args.more == False:
		cmd = 'cat -v %s | %s --path.data %s -f %s >> %s' \
		% (use_this_datafile, cfg['logstashbin'], cfg['tempdir'], feed.cfgfile, cmdlog)

	# logically, we could get here if args.silent == True and args.more == True
	# without building a command string!
	elif args.silent == True and args.more == True:
		raise Exception('unable to process request with both "silent" and "more" as command line arguments')

	logger.debug("cmd: %s" % (cmd))
	return cmd

def process_file(feed,filespec):
	""" cat <inputfile> | <logstash-program> -f <feed-configfile> """

	if feed.type == FT_S3_VIA_AWSCLI:
		return process_s3_file_using_awscli(feed,filespec)

	msg = "using the %s feed to process %s, please wait ... " % (feed.name,filespec)
	logger.info(msg)
	#sys.stdout.write("using the %s feed to process %s, please wait ... " % (feed.name,filespec))
	#sys.stdout.flush()

	# if feed.transform == True, 
	# then (1) copy data file to tempdir, (2) transform the copy, (3) update using the transformed copy
	use_this_datafile = preprocess_feed_file_as_required(feed,filespec)
	if use_this_datafile == None:
		return False

	cmd = build_command_for_processing_feed_file(feed,filespec)
	logger.debug("cmd: %s" % (cmd))
	retval = subprocess.call(cmd, shell=True)
	logger.debug("finished processing file")
	if retval != 0:
		msg = "file processing failure, command returned nonzero value %d" % (retval)
		logger.error(msg)
		#print "file processing failure"
		return False

	postprocess_feed_file_as_required(feed,filespec)
	# need to update the 'files_successfully_processed' table in the database
	# let the caller do that; make sure caller checks return code and follows up

	return True

def brag_about_success(feed,filespec):
	""" here is where we update the record in db of files successfully processed """

	try:
		conn = sqlite3.connect(cfg['dbfilespec'])
	except Exception, e:
		msg = "Connect fail for db %s: %s" % (cfg['dbfilespec'],str(e))
		logger.error(msg)
		return False

	c = conn.cursor()

	# get list of feeds and their properties
	sql_insert = "INSERT OR IGNORE INTO files_successfully_processed(filespec) VALUES ('%s')" \
	  % (filespec)

	logger.debug(sql_insert)

	try:
		c.execute(sql_insert)
	except Exception, e:
		msg = "Execute fail on db %s for query %s: %s" \
		 % (cfg['dbfilespec'],sql_insert,str(e))
		logger.error(msg)
		conn.close()
		return False

	try:
		conn.commit()
	except Exception, e:
		msg =  "Commit fail on db %s for query %s: %s" \
		% (cfg['dbfilespec'],sql_insert,str(e))
		logger.error(msg)
		conn.close()
		return False

	conn.close()
	return True

def is_valid_enabled_feed(feed):
	""" verify that feed is enabled and has a valid config file """
	logger.debug("verifying that feed {%s} is enabled and has a valid config file" % (feed.name))

	# if feed is not enabled, ignore it
	if not feed.enabled == True:
		msg = "skipping disabled feed %s" % (feed.name)
		logger.info(msg)
		return False

	# 1. verify that the config file is valid
	if not feed.is_valid_config_file():
		msg = 'input feed %s has invalid config file %s' % (feed.name,feed.cfgfile)
		logger.warning(msg)
		#warnings.warn(msg)
		return False

	return True

def find_files_by_feed_and_path(feed,p):
	""" find feed files by feed and path """

	logger.debug("finding files for feed {%s} (feed type: %d) using path {%s}" \
		% (feed.name,feed.type,p))
	# 2.1. generate a list of all filespecs for files that match the pattern
	if feed.type == FT_FILESYSTEM:
		files = p.find_files()
	elif feed.type == FT_S3_VIA_AWSCLI:
		files = p.find_files_on_s3_via_awscli()
	# otherwise, unsupported
	else:
		raise Exception("unrecognized feed type: %d" % (feed.type))

	logger.info("found files for path")
	logger.debug(files)
	return files

def process_feed_file(feed,f):
	""" process a feed file according to the feed type """

	# arguments to 'process_file' are a feed and a filespec
	if feed.type == FT_FILESYSTEM:
		return process_file(feed,f)
	elif feed.type == FT_S3_VIA_AWSCLI:
		return process_s3_file_using_awscli(feed,f)
	# otherwise, unsupported
	else:
		raise Exception("unrecognized feed type: %d" % (feed.type))

	return None

def process_feed(feed):
	""" fetch interesting paths associated with feed from db, then process those """

	logger.info("processing feed %s" % (feed.name))
	if not is_valid_enabled_feed(feed):
		return

	# 2. for each interesting path associated with this feed,
	for p in feed.fetch_interesting_paths():
		# is the path blacklisted? if so, skip
		logger.debug("processing interesting path: %s" % (p))
		# 2.1.1. for each file within the interesting path that match the pattern,
		for f in find_files_by_feed_and_path(feed,p):
			logger.debug("processing data file")
			# 2.1.1.1. check in db to see if the file has already been processed 
			if file_already_processed(f):
				# if it has, ignore and continue
				logger.info("skipping already processed file: %s" % (f))
				continue
			# otherwise process the file
			retval = process_feed_file(feed,f)
			# be sure to check the return value
			# returns True on success, False (or None) on fail
			if retval == True:
				# update the 'files_successfully_processed' table
				r = brag_about_success(feed,f)
				if not r == True:
					msg = "failed to update the files_successfully_processed table"
					logger.warning(msg)
					#warnings.warn(msg)
			else:
				logger.warning("process_file returned a non-True value")
			time.sleep(1)

def run_once(feed_name):
	""" fetch feeds and process them on time only """

	if feed_name == None:
		# process all enabled feeds
		feeds = fetch_feeds()
		for k,feed in feeds.iteritems():
			process_feed(feed)

		return

	# otherwise process single feed by name
	feed = fetch_a_feed(args.use)
	if feed == None:
		msg = "No logstash input feed found in gruppy db named %s" % (args.use)
		logger.error(msg)
		print msg
		sys.exit(1)

	process_feed(feed)


def run_continuously(polling_interval, feed_name):
	""" fetch feeds and process them, rinse and repeat, endlessly """
	# could/should assert that polling interval is set
	while(1):
		run_once(feed_name)
		logger.info("sleeping %d seconds until next polling time" % (polling_interval))
		time.sleep(polling_interval)


def show_version():
	""" show us what's what """

	print " %s version %s" % (APPNAME, VERSION)
	sys.exit(0)


init()

""" MAIN starts here """
if args.interval == None:
	logger.info("running once")
	run_once(args.use)  # args.use is name of an input feed
	logger.info("processing complete")

else:
	logger.info("running continuously with polling interval = %d" % (args.interval))
	run_continuously(args.interval,args.use)
